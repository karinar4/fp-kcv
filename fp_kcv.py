# -*- coding: utf-8 -*-
"""fp-kcv.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KBoFadEpwSCYWW3OUomFUW2iaQ6WV5wf

# Introduction

Diabetes is a metabolic disease caused by elevated levels of blood glucose, leading to significant damage to vital organ systems. According to the World Health Organization, approximately 422 million people are diabetic, with 1.5 million deaths directly attributed to the condition.

While long-term risks can be mitigated through monitoring and treatment, short-term dangers can be severe and potentially fatal for diabetic patients.

Fortunately, in most cases, diagnosing diabetes is relatively inexpensive and accessible. In order to make it more accessible and free of cost, we aim to uncover methods to predict an individual's likelihood of suffering from diabetes based on various characteristics i.e. hypertension, age, gender, etc.

To start, we discovered a dataset from Kaggle.com to train and explore diabetes, aiming to uncover patterns, correlations, and insights that can inform both medical understanding and eventually predict whether someone has diabetes or not.

## Data Description

**Objective:** The objective of this project is to develop a robust model for predicting the likelihood of diabetes in patients based on their medical history and demographic information. Such predictions can be immensely valuable for healthcare professionals in identifying individuals who may be at risk of developing diabetes.

**Dataset:** The dataset used for this project is the Diabetes Prediction Dataset, which comprises a comprehensive collection of medical and demographic data from patients, along with their diabetes status (positive or negative). The dataset encompasses several essential features including age, gender, body mass index (BMI), hypertension, heart disease, smoking history, HbA1c level, and blood glucose level.

To access the **complete dataset**, please visit the following page: https://www.kaggle.com/datasets/iammustafatz/diabetes-prediction-dataset

## Domain Knowledge

1. `Age`: Age is important factor in predicting diabetes risk. As people get older, they become more likely to develop diabetes. This is because things like being less active, changes in hormones, and having other health problems.

2.  `Gender`: Gender can play a role in diabetes risk, although the effect may vary. For example, women with a history of gestational diabetes (diabetes during pregnancy) have a higher risk of developing type 2 diabetes later in life. Additionally, some studies have suggested that men may have a slightly higher risk of diabetes compared to women.

3. `Body Mass Index(BMI)`: BMI measures body fat based on height and weight. A higher BMI means a higher risk of type 2 diabetes. Too much fat, especially around the waist, can make it hard for the body to control blood sugar.

4. `Hypertension`: High blood pressure often comes with diabetes. They both have similar causes and make each other worse. Having high blood pressure raises the chances of getting type 2 diabetes, and having diabetes raises the chances of getting high blood pressure.

5. `Heart Disease`: Diabetes and heart disease often go together. Having one makes it more likely to get the other because they share many of the same risk factors, like being overweight or having high cholesterol.

6. `Smoking History`: Smoking can increase the chances of getting type 2 diabetes. It messes with how the body handles insulin and glucose. But if someone quits smoking, they can lower their risk of getting diabetes.

7. `HbA1c Level`: HbA1c (glycated hemoglobin) shows the average blood glucose level over a few months. If the HbA1c level is high, it means blood sugar hasn't been well controlled, raising the risk of diabetes and its complications.

8. `Blood Glucose Level`: This shows how much sugar is in the blood at a given time. If it's too high, especially when someone hasn't eaten or after they eat carbs, it can mean they're at risk for diabetes. Keeping an eye on blood sugar levels is important for managing diabetes.

These features, when combined and analyzed with appropriate statistical and machine learning techniques, can help in predicting an individual's risk of developing diabetes.

# Setting Up
This section contains the initial process carried out before starting to analyze the data. This includes steps such as setting up the work environment, downloading or loading data, importing necessary software or libraries, and preparing other tools and resources necessary to properly analyze the data.
"""

!pip install catboost

# General
import numpy as np
import pandas as pd
import json

# Graph
import matplotlib.pyplot as plt
import seaborn as sns

# Machine Learning
from sklearn.model_selection import train_test_split
from catboost import CatBoostClassifier, Pool, cv
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# Preprocessing utilities
from sklearn.preprocessing import StandardScaler

# Sampling methods
from imblearn.over_sampling import SMOTE

# General Metrics
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score, roc_curve

# Serialize and de-serialize a python object structure
import pickle

"""## Importing Libraries
- `numpy` and `pandas` are utilized for data manipulation tasks.
- `catboost` libraries are imported to be our main prediction model. Other classifiers such as `XGBoost`, `LogisticRegression`, and `RandomForestClassifier` are imported as a comparison.
- `SMOTE` is brought in for oversampling purposes, while `StandardScaler` is used for standardization for numerical data.
- Metrics functions including `accuracy_score`, `precision_score`, and `confusion_matrix` are imported for model evaluation.
- `matplotlib` and `seaborn`is employed for data visualization
"""

!pip install -q kaggle

cred = {"username":"karinarahmawati","key":"d414ff81116181f1d8dae59293ba8470"}

!mkdir ~/.kaggle/
!touch ~/.kaggle/kaggle.json

api_token = cred

with open('/root/.kaggle/kaggle.json', 'w') as file:
    json.dump(api_token, file)

!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d iammustafatz/diabetes-prediction-dataset

!unzip "diabetes-prediction-dataset.zip"

df = pd.read_csv("diabetes_prediction_dataset.csv")

"""# Exploratory Data Analysis

This section involves performing exploratory data analysis to gain insights into the dataset. Through data description, visualization and correlation map, we explore the distributions, relationships, and characteristics of the variables.

## Data Description
To describe the outline of the data such as shape, data types, unique values and percentage of missing values.
"""

df.head()

df.shape

# Concatenates data types, unique values, and percentage of NaN values
info = pd.concat([df.dtypes, df.nunique(), df.isnull().sum()*100/len(df)], axis=1)
info.columns = ['dtypes', 'nunique', 'nan %']
info

df.describe().transpose()

df.info()

"""## Data Visualization

There are several categorical and numerical columns that will be explored from this dataset,

### Data Visualization

#### Categorical Data:

1. **Gender:** Female or Male

2. **Hypertension:** Yes (1) or No (0)

3. **Heart Disease:** Yes (1) or No (0)

4. **Smoking History:**
   - Never
   - Ever
   - Current
   - Former
   - No Information Available

5. **Diabetes:** Yes (1) or No (0)

#### Numerical Data:

1. **Age**
2. **BMI (Body Mass Index)**
3. **HbA1c Level**
4. **Blood Glucose Level**

### Categorical Data

#### Gender
"""

df['gender'].value_counts()

# Plot a count of gender categorized by diabetes status
plt.figure(figsize=(5,5))
sns.countplot(x='gender', data=df, palette='crest', hue='diabetes').set(title='Count Plot of Gender')
plt.show()

"""#### Hypertension"""

df['hypertension'].value_counts()

# Plot a count of hypertension cases categorized by diabetes status
plt.figure(figsize=(5,5))
sns.countplot(x='hypertension', data=df, palette='crest', hue='diabetes').set(title='Count Plot of Hypertension')
plt.show()

"""#### Heart Disease"""

df['heart_disease'].value_counts()

# Plot a count of heart disease cases categorized by diabetes status
plt.figure(figsize=(5,5))
sns.countplot(x='heart_disease', data=df, palette='crest', hue='diabetes').set(title='Count Plot of Heart Disease')
plt.show()

"""#### Smoking History"""

df['smoking_history'].value_counts()

# Plot a count of smoking history cases categorized by diabetes status
plt.figure(figsize=(7,5))
sns.countplot(x='smoking_history', data=df, palette='crest', hue='diabetes').set(title='Count Plot of Smoking History')
plt.show()

"""#### Diabetes"""

df['diabetes'].value_counts()

# Plot a count distribution of 'diabetes' variable
plt.figure(figsize=(4,5))
sns.countplot(x='diabetes', data=df, palette='crest').set(title='Count Plot of Diabetes')
plt.show()

"""Observing the `diabetes` frequency data offered from the dataset, it can be inferred that there's a huge disparity between the number of individuals who are not diabetic and who suffers from diabetes. Consequently, the training model will be biased towards predicting individuals that don't have diabetes and struggles to predict that are diabetic. In the later section of this article, this problem can be tackled with oversampling the training data with SMOTE.

### Numerical Data

#### Age
"""

# Histogram and boxplot for the 'age' variable divided by diabetes status
fig, ax = plt.subplots(figsize=(10,5),nrows=1, ncols=2)
sns.histplot(data=df,x='age', hue='diabetes',ax=ax[0], palette='crest')
sns.boxplot(data=df, y='age', ax=ax[1], palette='crest', width = 0.4)
plt.tight_layout()
plt.show()

"""#### BMI"""

# Histogram and boxplot for the 'bmi' variable divided by diabetes status
fig, ax = plt.subplots(figsize=(10,5),nrows=1, ncols=2)
sns.histplot(data=df,x='bmi', hue='diabetes',ax=ax[0], palette='crest', bins=20)
sns.boxplot(data=df, y='bmi', ax=ax[1], palette='crest', width = 0.4)
plt.tight_layout()
plt.show()

"""#### HbA1c Level"""

fig, ax = plt.subplots(figsize=(10,5),nrows=1, ncols=2)
sns.histplot(data=df,x='HbA1c_level', hue='diabetes',ax=ax[0], palette='crest', bins=20)
sns.boxplot(data=df, y='HbA1c_level', ax=ax[1], palette='crest', width = 0.4)
plt.tight_layout()
plt.show()

"""#### Blood Glucose Level"""

fig, ax = plt.subplots(figsize=(10,5),nrows=1, ncols=2)
sns.histplot(data=df,x='blood_glucose_level', hue='diabetes',ax=ax[0], palette='crest', bins=20)
sns.boxplot(data=df, y='blood_glucose_level', ax=ax[1], palette='crest', width = 0.4)
plt.tight_layout()
plt.show()

"""## Correlation Map

Correlation maps will explore relationships within a dataset containing both numerical and categorical features, particularly focusing on diabetes-related factors. `The first correlation graph` presents a comprehensive correlation matrix heatmap depicting correlations among all features, aiding in identifying patterns and associations. `The second correlation graph` specifically highlights correlations of individual features with the target variable, diabetes, providing insights into which factors might have stronger relationships with the presence of diabetes.
"""

#  Dataframe with numerical features
df_num_features = df[['age', 'bmi', 'HbA1c_level', 'blood_glucose_level', 'diabetes']]

# Dataframe with binary features
df_bin_features = df[['hypertension','heart_disease']]

# Using pandas get_dummies method for the categorical variables
df_cat_features_encoded = pd.concat([pd.get_dummies(df[cat_feature],prefix=cat_feature) for cat_feature in df[['gender','smoking_history']].columns.tolist()],axis=1)

# Concatenating all dataframes
df_all_features = pd.concat([df_num_features, df_bin_features, df_cat_features_encoded], axis=1)

df_all_features.head()

# Correlation 1
plt.figure(figsize=(10,7))
sns.heatmap(df_all_features.corr(), cmap="crest", annot=True, fmt='.2f')
plt.title("Correlation Matrix Heatmap")
plt.show()

# Correlation 2
# Create a heatmap of the correlations with the target column
corr = df_all_features.corr()
target_corr = corr['diabetes'].drop('diabetes')

# Sort correlation values in descending order
target_corr_sorted = target_corr.sort_values(ascending=False)

plt.figure(figsize=(8, 6))
sns.set(font_scale=0.8)
sns.set_style("white")
sns.set_palette("PuBuGn_d")
sns.heatmap(target_corr_sorted.to_frame(), cmap="crest", annot=True, fmt='.2f')
plt.title('Correlation with Diabetes')
plt.show()

"""# Data Preprocessing

To ensure the quality of exploratory data analysis and achieve optimal results for the data model, data preprocessing is needed. In data preprocessing, several essential steps ensure the quality and integrity of the dataset.

- Encode ordinal values to numerical equivalents, allowing algorithms to interpret them effectively.
- Address the correct data types.
- Null and duplicate values are addressed through techniques like imputation or deletion to prevent biased analyses.
- Remove unnecessary values.
- Standardization to scale features appropriately.

These preprocessing steps collectively lay a robust foundation for accurate and reliable data analysis in the field of data science.
"""

# Check duplicate values
df.duplicated().sum()

# Drop the duplicated values
df = df.drop_duplicates()

df['gender'].value_counts()

# Remove Unneccessary value [0.00195%]
df = df[df['gender'] != 'Other']

df.shape

df['age'].value_counts()

# Change the data type
df['age'] = df['age'].astype(int)

columns = ['bmi']

for column_name in columns:
    Q1 = np.percentile(df[column_name], 25, interpolation='midpoint')
    Q3 = np.percentile(df[column_name], 75, interpolation='midpoint')

    IQR = Q3 - Q1
    low_lim = Q1 - 1.5 * IQR
    up_lim = Q3 + 1.5 * IQR

    # Delete outliers in the specified column
    df = df[(df[column_name] >= low_lim) & (df[column_name] <= up_lim)]

df.shape

# Standardize numerical columns
scaler = StandardScaler()
df_scaled = df.copy()
num_column = ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level']
scaler.fit(df[num_column].values)

pickle.dump(scaler, open("scaler.pkl", "wb"))

df_scaled[num_column] = scaler.transform(df[num_column].values)
df_scaled

# Encoding categorical columns
df_scaled['gender'] = df_scaled['gender'].astype('category')
df_scaled['gender'] = df_scaled['gender'].cat.codes
df_scaled['smoking_history'] = df_scaled['smoking_history'].astype('category')
df_scaled['smoking_history'] = df_scaled['smoking_history'].cat.codes

df['gender'].value_counts()

df_scaled['gender'].value_counts()

df['smoking_history'].value_counts()

df_scaled['smoking_history'].value_counts()

"""# Prediction and Model Fitting

To tackle our classification problem, we've chosen the **CatBoost classifier** for various reasons. Based on their website, [catboost.ai](https://catboost.ai):

> CatBoost is an algorithm for gradient boosting on decision trees. It is developed by Yandex researchers and engineers, and is used for search, recommendation systems, personal assistant, self-driving cars, weather prediction and many other tasks at Yandex and in other companies, including CERN, Cloudflare, Careem taxi. It is in open-source and can be used by anyone.

In comparison to other classifiers in the market, CatBoost consistently outperforms its competitors such as XGBoost and Random Forests. One of its significant advantages is the short training time for decision trees, often completing training with optimal results. This efficiency significantly benefits for our team, enabling us to train models with default parameters quickly while still achieving optimal prediction accuracy.
"""

# Define data training and target
X = df_scaled.drop(['diabetes'], axis=1)
y = df_scaled['diabetes']

# Training/Test split and oversampling using SMOTE
OS = SMOTE(random_state=25)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
X_train, y_train = OS.fit_resample(X_train, y_train)

"""To address the issue of class imbalance observed during the exploratory data analysis (EDA) phase, implementing **SMOTE (Synthetic Minority Over-sampling Technique)** to oversample the minority class in the training data can be an effective strategy.

By generating synthetic samples for the minority class, SMOTE helps mitigate the imbalance and ensures that the model is trained on a more representative dataset. This approach not only prevents the model from being biased towards the majority class but also enhances the model's ability to generalize to unseen data. By balancing the class distribution in the training data, SMOTE contributes to improving the overall quality and reliability of the model's predictions, particularly for the minority class.
"""

# Model Fitting
cb = CatBoostClassifier()
cb.fit(X_train, y_train)

pickle.dump(cb, open("model.pkl", "wb"))

# Classifier predicting data testing
y_pred = cb.predict(X_test)

"""### Accuracy Score"""

# Accuracy score
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

"""The CatBoost classifier achieved around a 97% accuracy score, which is a significant number, indicating that the model is highly effective at correctly classifying instances in the dataset.

However, accuracy alone may not provide a complete picture of the model's performance, especially in scenarios where class distribution is imbalanced or where different types of errors carry varying degrees of importance.

### Classification Report
"""

print(classification_report(y_test, y_pred))

"""The model achieves exceptionally high precision, recall, and F1-score for both classes. For class 0 (absence of the condition), the precision is 0.97, recall is 1.00, and F1-score is 0.98, indicating the model's ability to accurately identify instances without the condition.

Similarly, for class 1 (presence of the condition), the precision is 0.94, recall is 0.70, and F1-score is 0.80, showcasing the model's decent proficiency in correctly classifying instances with the condition.

The overall accuracy of the model is 97%, with macro and weighted average metrics also reflecting high scores, indicating consistent performance across both classes. This suggests that the model effectively discriminates between the two classes and demonstrates robustness in its predictions.

### Confusion Matrix
"""

# Displaying Confusion Matrix
cm = confusion_matrix(y_test, y_pred, labels=cb.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=cb.classes_)
disp.plot()
plt.show()

# Showing true results of y_test split
y_test.value_counts()

"""From the confusion matrix that our team gathered, it can be inferred that the model scored 17000+ True Negatives and True Positives, while the remaining 1200+ predictions were False Positives and False Negatives.

Our team can conclude that the model can consistently predicts individuals who has a diabetes or not.

### Feature Importance
"""

# Gather Feature Importance from CatBoost classifier
fi = cb.get_feature_importance(prettified=True)

# Categorize features with importance less than 1 as others
fi_df = fi[fi['Importances'] >= 1].copy()
new_row = pd.DataFrame(data={
    'Feature Id' : ['others'],
    'Importances' : [fi['Importances'][fi['Importances'] < 1].sum() ]
})
fi_df = pd.concat([fi_df, new_row])

# Visualize
fig, axes = plt.subplots(figsize = (6,6))
plt.subplots_adjust(1,0.5,2,5)
axes.set_title('Feature Importance')
plt.pie(fi_df['Importances'], labels=fi_df['Feature Id'], radius=0.89, autopct='%1.1f%%')
plt.legend(loc = 'upper left')
plt.show()

"""From the feature importance gathered from the model, HbA1c or hemoglobin and blood glucose levels holds an important role in determining whether an individual is diabetic or not. This confirms the real-world application of diabetes check-ups, where monitoring blood glucose and hemoglobin levels helps determine diabetic status.

# Conclusion

Based on the results of the data analysis that has been carried out, the following conclusions can be drawn.
1. Diabetes prediction data that has been explored and analyzed using statistical methods produces important information regarding characteristics and relationships between features.
2. Diabetes prediction data that has gone through the preprocessing stage by handling missing values, handling duplicate data, and removing noise, has much better quality.
3. From feature importance
it can be inferred that
Hemoglobin and blood glucose are both important features that determine diabetes or not.
4. Several model screening experiments have been carried out, from Linear Regression, Random Forest, Catboost and XGBoost, it was found that the Catboost model has the highest accuracy value, namely 0.9722 or equivalent to 97%

# Appendix: Other Classifier (Comparison)

## XGBoost
"""

# Create and train the XGBoost Classifier
xgb = XGBClassifier()
xgb.fit(X_train, y_train)

# Predict on the test set
y_pred = xgb.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print("XGBoost Model Accuracy:", accuracy)
print("Classification Report:\n", report)
cm = confusion_matrix(y_test, y_pred, labels=xgb.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=xgb.classes_)
disp.plot()
plt.show()

"""## Logistic Regression

"""

logistic_model = LogisticRegression()
logistic_model.fit(X_train, y_train)

# Make predictions on the test set
logistic_predictions = logistic_model.predict(X_test)

# Calculate AUC
logistic_auc = roc_auc_score(y_test, logistic_predictions)

# Generate ROC curve
fpr, tpr, _ = roc_curve(y_test, logistic_predictions)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, logistic_predictions)
print(f"Logistic Regression Accuracy: {accuracy:.2f}")
print("Logistic Regression Classification Report:")
print(classification_report(y_test, logistic_predictions))

# Displaying Confusion Matrix
cm = confusion_matrix(y_test, y_pred, labels=logistic_model.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logistic_model.classes_)
disp.plot()
plt.show()

"""## Random Forest"""

# Create Random Forest model
random_forest_model = RandomForestClassifier()
random_forest_model.fit(X_train, y_train)

# Make predictions on the test set
random_forest_predictions = random_forest_model.predict(X_test)

# Calculate AUC
random_forest_auc = roc_auc_score(y_test, random_forest_predictions)

# Generate ROC curve
fpr_rf, tpr_rf, _ = roc_curve(y_test, random_forest_predictions)

# Evaluate the model's performance
accuracy_rf = accuracy_score(y_test, random_forest_predictions)
print(f"Random Forest Accuracy: {accuracy_rf}")
print("Random Forest Classification Report:")
print(classification_report(y_test, random_forest_predictions))

# Displaying Confusion Matrix
cm_rf = confusion_matrix(y_test, random_forest_predictions, labels=random_forest_model.classes_)
disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=random_forest_model.classes_)
disp_rf.plot()
plt.show()